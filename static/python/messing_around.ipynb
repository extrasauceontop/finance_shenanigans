{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the endpoint\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define parameters\n",
    "param_dict = {\"action\": \"getcompany\",\n",
    "             \"CIK\": \"MSFT\",\n",
    "             \"type\": \"10-k\",\n",
    "             #\"dateb\": \"20190101\",\n",
    "             \"owner\": \"exclude\",\n",
    "             \"start\": \"\",\n",
    "             \"output\": \"atom\",\n",
    "             \"count\": \"100\"}\n",
    "\n",
    "# define response\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the text\n",
    "def restore_windows_1252_characters(restore_string):\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode(\"windows-1252\")\n",
    "        except UnicodeDecodeError:\n",
    "            return \" \"\n",
    "    \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the url to a specific text file\n",
    "new_html_text = r\"https://www.sec.gov/Archives/edgar/data/789019/000156459020034944/0001564590-20-034944.txt\"\n",
    "\n",
    "# get the accession number\n",
    "url_split = new_html_text.split(\"/\")\n",
    "accession_number = url_split[-1].replace(\".txt\", \"\")\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# parse the response\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new dictionary to house filings\n",
    "master_filings_dict = {}\n",
    "\n",
    "# add the key to the dict and add a new level\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# add the next levels\n",
    "master_filings_dict[accession_number][\"sec_header_content\"] = {}\n",
    "master_filings_dict[accession_number][\"filing_documtents\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the sec-header document\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "#store the sec header content inside the dictionary\n",
    "master_filings_dict[accession_number][\"sec_header_content\"][\"sec_header_code\"] = sec_header_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize master document dictionary\n",
    "master_document_dict = {}\n",
    "\n",
    "# loop through each document in the filing\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define my document id\n",
    "    document_id = filing_document.type.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    if document_id == \"XML\":\n",
    "        break\n",
    "    \n",
    "    # document sequence\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document filename\n",
    "    document_filename = filing_document.filename.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document description\n",
    "    document_description = filing_document.description.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # insert the key\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts of the document\n",
    "    master_document_dict[document_id][\"document_sequence\"] = document_sequence\n",
    "    master_document_dict[document_id][\"document_filename\"] = document_filename\n",
    "    master_document_dict[document_id][\"document_description\"] = document_description\n",
    "    \n",
    "    # add the document content itself\n",
    "    master_document_dict[document_id][\"document_code\"] = filing_document.extract()\n",
    "    \n",
    "    # get all the text in the document\n",
    "    filing_doc_text = filing_document.find(\"text\").extract()\n",
    "    \n",
    "    # get all thematic breaks\n",
    "    all_thematic_breaks = filing_doc_text.find_all(attrs={\"style\": \"page-break-after:always;\"})\n",
    "    \n",
    "    # convert all the breaks into strings\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document for being split\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "    if len(all_thematic_breaks) > 0:\n",
    "        \n",
    "        # creates our pattern\n",
    "        regex_delimited_pattern = \"|\".join(map(re.escape, all_thematic_breaks))\n",
    "        \n",
    "        # split the document along the thematic breaks\n",
    "        split_filing_string = re.split(regex_delimited_pattern, filing_doc_string)\n",
    "        \n",
    "        # store the document in the dictionary\n",
    "        master_document_dict[document_id][\"pages_code\"] = split_filing_string\n",
    "        \n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "        master_document_dict[document_id][\"pages_code\"] = [filing_doc_string]\n",
    "        \n",
    "# store the documents in the master_filing_dictionary\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = master_document_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number][\"filing_documents\"]\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # grab all the pages for each document\n",
    "    document_pages = filing_documents[document_id][\"pages_code\"]\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(document_pages)\n",
    "    \n",
    "    # initialize some dictionaries\n",
    "    repaired_pages = {}\n",
    "    normalized_text = {}\n",
    "    \n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser to repair it\n",
    "        page_soup = BeautifulSoup(page, \"html5\")\n",
    "        \n",
    "        # grab the text from each page\n",
    "        page_text = page_soup.html.body.get_text(\" \", strip = True)\n",
    "        \n",
    "        # normalize the text\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize(\"NFKD\", page_text))\n",
    "        \n",
    "        # additional cleaning\n",
    "        page_text_norm = page_text_norm.replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
    "        \n",
    "        # define our page number\n",
    "        page_number = index+1\n",
    "        \n",
    "        # add normalized text to the dictionary\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        \n",
    "        # add the repaired html code to the dictionary\n",
    "        repaired_pages[page_number] = page_soup\n",
    "    \n",
    "    # add the normalized text dictionary to the master filing dictionary\n",
    "    filing_documents[document_id][\"page_normalized_text\"] = normalized_text\n",
    "    \n",
    "    # add the repaired pages to the master filing dictionary\n",
    "    filing_documents[document_id][\"pages_code\"] = repaired_pages\n",
    "    \n",
    "    # add the page numbers we generate\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    filing_documents[document_id][\"page_numbers_generated\"] = gen_page_numbers\n",
    "\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = filing_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "45",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-b628c4f6e55d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmaster_filings_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccession_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filing_documents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"10-K\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"page_normalized_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 45"
     ]
    }
   ],
   "source": [
    "master_filings_dict[accession_number][\"filing_documents\"][\"10-K\"][\"page_normalized_text\"][45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
