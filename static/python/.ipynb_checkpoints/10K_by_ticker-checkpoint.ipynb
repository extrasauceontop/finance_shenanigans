{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the endpoint\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define parameters\n",
    "param_dict = {\"action\": \"getcompany\",\n",
    "             \"CIK\": \"AMZN\",\n",
    "             \"type\": \"10-k\",\n",
    "             \"owner\": \"exclude\",\n",
    "             \"output\": \"atom\"}\n",
    "\n",
    "# define response\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "# find url to get to most recent 10-K filings\n",
    "new_url = soup.find(\"entry\").find(\"filing-href\").text.strip().replace(\"-index.htm\", \".txt\")\n",
    "\n",
    "response = requests.get(url = new_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the text\n",
    "def restore_windows_1252_characters(restore_string):\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode(\"windows-1252\")\n",
    "        except UnicodeDecodeError:\n",
    "            return \" \"\n",
    "    \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the url to go to the specific text file\n",
    "new_html_text = new_url\n",
    "\n",
    "# get the accession number\n",
    "url_split = new_html_text.split(\"/\")\n",
    "accession_number = url_split[-1].replace(\".txt\", \"\")\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# parse the response\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new dictionary to house filings\n",
    "master_filings_dict = {}\n",
    "\n",
    "# add the key to the dict and add a new level\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# add the next levels\n",
    "master_filings_dict[accession_number][\"sec_header_content\"] = {}\n",
    "master_filings_dict[accession_number][\"filing_documtents\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the sec-header document\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "#store the sec header content inside the dictionary\n",
    "master_filings_dict[accession_number][\"sec_header_content\"][\"sec_header_code\"] = sec_header_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-K\n",
      "EX-4.6\n"
     ]
    }
   ],
   "source": [
    "# initialize master document dictionary\n",
    "master_document_dict = {}\n",
    "\n",
    "# loop through each document in the filing\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define my document id\n",
    "    document_id = filing_document.type.find(text=True, recursive = False).strip()\n",
    "    print(document_id)\n",
    "    \n",
    "    if document_id != \"10-K\":\n",
    "        break\n",
    "    \n",
    "    # document sequence\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document filename\n",
    "    document_filename = filing_document.filename.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document description\n",
    "    document_description = filing_document.description.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # insert the key\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts of the document\n",
    "    master_document_dict[document_id][\"document_sequence\"] = document_sequence\n",
    "    master_document_dict[document_id][\"document_filename\"] = document_filename\n",
    "    master_document_dict[document_id][\"document_description\"] = document_description\n",
    "    \n",
    "    # add the document content itself\n",
    "    master_document_dict[document_id][\"document_code\"] = filing_document.extract()\n",
    "    \n",
    "    # get all the text in the document\n",
    "    filing_doc_text = filing_document.find(\"text\").extract()\n",
    "    \n",
    "    # get all thematic breaks\n",
    "    all_thematic_breaks = filing_doc_text.find_all(\"hr\",{\"style\": \"page-break-after:always\"})\n",
    "    \n",
    "    # convert all the breaks into strings\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document for being split\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "    if len(all_thematic_breaks) > 0:\n",
    "        \n",
    "        # creates our pattern\n",
    "        regex_delimited_pattern = \"|\".join(map(re.escape, all_thematic_breaks))\n",
    "        \n",
    "        # split the document along the thematic breaks\n",
    "        split_filing_string = re.split(regex_delimited_pattern, filing_doc_string)\n",
    "        \n",
    "        # store the document in the dictionary\n",
    "        master_document_dict[document_id][\"pages_code\"] = split_filing_string\n",
    "        \n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "        master_document_dict[document_id][\"pages_code\"] = [filing_doc_string]\n",
    "        \n",
    "# store the documents in the master_filing_dictionary\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = master_document_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number][\"filing_documents\"]\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # grab all the pages for each document\n",
    "    document_pages = filing_documents[document_id][\"pages_code\"]\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(document_pages)\n",
    "    \n",
    "    # initialize some dictionaries\n",
    "    repaired_pages = {}\n",
    "    normalized_text = {}\n",
    "    \n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser to repair it\n",
    "        page_soup = BeautifulSoup(page, \"html5\")\n",
    "        \n",
    "        # grab the text from each page\n",
    "        page_text = page_soup.html.body.get_text(\" \", strip = True)\n",
    "        \n",
    "        # normalize the text\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize(\"NFKD\", page_text))\n",
    "        \n",
    "        # additional cleaning\n",
    "        page_text_norm = page_text_norm.replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
    "        \n",
    "        # define our page number\n",
    "        page_number = index+1\n",
    "        \n",
    "        # add normalized text to the dictionary\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        \n",
    "        # add the repaired html code to the dictionary\n",
    "        repaired_pages[page_number] = page_soup\n",
    "    \n",
    "    # add the normalized text dictionary to the master filing dictionary\n",
    "    filing_documents[document_id][\"page_normalized_text\"] = normalized_text\n",
    "    \n",
    "    # add the repaired pages to the master filing dictionary\n",
    "    filing_documents[document_id][\"pages_code\"] = repaired_pages\n",
    "    \n",
    "    # add the page numbers we generate\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    filing_documents[document_id][\"page_numbers_generated\"] = gen_page_numbers\n",
    "\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = filing_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table of Contents Other Income (Expense), Net Other income (expense), net, consists primarily of adjustments to and gains on equity securities of $ 18 million , $ 145 million , and $ 231 million in 2017 , 2018 , and 2019 , equity warrant valuation gains (losses) of $ 109 million , $( 131 ) million , and $ 11 million in 2017 , 2018 , and 2019 , and foreign currency gains (losses) of $ 247 million , $( 206 ) million , and $( 20 ) million in 2017 , 2018 , and 2019 . Income Taxes Income tax expense includes U.S. (federal and state) and foreign income taxes. Certain foreign subsidiary earnings are subject to U.S. taxation under the U.S. Tax Act, which also repeals U.S. taxation on the subsequent repatriation of those earnings. We intend to invest substantially all of our foreign subsidiary earnings, as well as our capital in our foreign subsidiaries, indefinitely outside of the U.S. in those jurisdictions in which we would incur significant, additional costs upon repatriation of such amounts. Deferred income tax balances reflect the effects of temporary differences between the carrying amounts of assets and liabilities and their tax bases and are stated at enacted tax rates expected to be in effect when taxes are actually paid or recovered. Deferred tax assets are evaluated for future realization and reduced by a valuation allowance to the extent we believe they will not be realized. We consider many factors when assessing the likelihood of future realization of our deferred tax assets, including our recent cumulative loss experience and expectations of future earnings, capital gains and investment in such jurisdiction, the carry-forward periods available to us for tax reporting purposes, and other relevant factors. We utilize a two-step approach to recognizing and measuring uncertain income tax positions (tax contingencies). The first step is to evaluate the tax position for recognition by determining if the weight of available evidence indicates it is more likely than not the position will be sustained on audit, including resolution of related appeals or litigation processes. The second step is to measure the tax benefit as the largest amount which is more than 50% likely of being realized upon ultimate settlement. We consider many factors when evaluating our tax positions and estimating our tax benefits, which may require periodic adjustments and which may not accurately forecast actual outcomes. We include interest and penalties related to our tax contingencies in income tax expense. Fair Value of Financial Instruments Fair value is defined as the price that would be received to sell an asset or paid to transfer a liability in an orderly transaction between market participants at the measurement date. To increase the comparability of fair value measures, the following hierarchy prioritizes the inputs to valuation methodologies used to measure fair value: Level 1 — Valuations based on quoted prices for identical assets and liabilities in active markets. Level 2 — Valuations based on observable inputs other than quoted prices included in Level 1, such as quoted prices for similar assets and liabilities in active markets, quoted prices for identical or similar assets and liabilities in markets that are not active, or other inputs that are observable or can be corroborated by observable market data. Level 3 — Valuations based on unobservable inputs reflecting our own assumptions, consistent with reasonably available assumptions made by other market participants. These valuations require significant judgment. For our cash, cash equivalents, or marketable securities, we measure the fair value of money market funds and certain marketable equity securities based on quoted prices in active markets for identical assets or liabilities. Other marketable securities were valued either based on recent trades of securities in inactive markets or based on quoted market prices of similar instruments and other significant inputs derived from or corroborated by observable market data. We did not hold significant amounts of cash, cash equivalents, restricted cash, or marketable securities categorized as Level 3 assets as of December 31, 2018 and 2019 . We hold equity warrants giving us the right to acquire stock of other companies. As of December 31, 2018 and 2019 , these warrants had a fair value of $ 440 million and $ 669 million , and are recorded within “Other assets” on our consolidated balance sheets. These assets are primarily classified as Level 2 assets. Cash and Cash Equivalents We classify all highly liquid instruments with an original maturity of three months or less as cash equivalents. 45'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_filings_dict[accession_number][\"filing_documents\"][\"10-K\"][\"page_normalized_text\"][45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/1018724/000101872420000004/0001018724-20-000004.txt'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythondata] *",
   "language": "python",
   "name": "conda-env-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
