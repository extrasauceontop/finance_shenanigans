{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the endpoint\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define parameters\n",
    "param_dict = {\"action\": \"getcompany\",\n",
    "             \"CIK\": \"ULTA\",\n",
    "             \"type\": \"10-k\",\n",
    "             \"owner\": \"exclude\",\n",
    "             \"output\": \"atom\"}\n",
    "\n",
    "# define response\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "# find url to get to most recent 10-K filings\n",
    "new_url = soup.find(\"entry\").find(\"filing-href\").text.strip().replace(\"-index.htm\", \".txt\")\n",
    "\n",
    "response = requests.get(url = new_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the text\n",
    "def restore_windows_1252_characters(restore_string):\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode(\"windows-1252\")\n",
    "        except UnicodeDecodeError:\n",
    "            return \" \"\n",
    "    \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the url to go to the specific text file\n",
    "new_html_text = new_url\n",
    "\n",
    "# get the accession number\n",
    "url_split = new_html_text.split(\"/\")\n",
    "accession_number = url_split[-1].replace(\".txt\", \"\")\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# parse the response\n",
    "soup = BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new dictionary to house filings\n",
    "master_filings_dict = {}\n",
    "\n",
    "# add the key to the dict and add a new level\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# add the next levels\n",
    "master_filings_dict[accession_number][\"sec_header_content\"] = {}\n",
    "master_filings_dict[accession_number][\"filing_documtents\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the sec-header document\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "#store the sec header content inside the dictionary\n",
    "master_filings_dict[accession_number][\"sec_header_content\"][\"sec_header_code\"] = sec_header_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-K\n",
      "<a href=\"#Toc\"><span style=\"font-family:'Times New Roman';font-size:10pt;font-style:normal;font-weight:normal;text-align:left;\">Table of Contents</span></a>\n",
      "EX-4\n"
     ]
    }
   ],
   "source": [
    "# initialize master document dictionary\n",
    "master_document_dict = {}\n",
    "\n",
    "# loop through each document in the filing\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define my document id\n",
    "    document_id = filing_document.type.find(text=True, recursive = False).strip()\n",
    "    print(document_id)\n",
    "    \n",
    "    if document_id[:4] != \"10-K\":\n",
    "        break\n",
    "    \n",
    "    # document sequence\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document filename\n",
    "    document_filename = filing_document.filename.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # document description\n",
    "    document_description = filing_document.description.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    # insert the key\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts of the document\n",
    "    master_document_dict[document_id][\"document_sequence\"] = document_sequence\n",
    "    master_document_dict[document_id][\"document_filename\"] = document_filename\n",
    "    master_document_dict[document_id][\"document_description\"] = document_description\n",
    "    \n",
    "    # add the document content itself\n",
    "    master_document_dict[document_id][\"document_code\"] = filing_document.extract()\n",
    "    \n",
    "    # get all the text in the document\n",
    "    filing_doc_text = filing_document.find(\"text\").extract()\n",
    "    \n",
    "    # get all thematic breaks\n",
    "    #use try, except to check for multiple formats\n",
    "    \n",
    "    try:\n",
    "        all_thematic_breaks = filing_doc_text.find_all(\"hr\")\n",
    "        print(all_thematic_breaks[0])\n",
    "    except Exception:\n",
    "        # trying another possible format\n",
    "        try:\n",
    "            all_thematic_breaks = filing_doc_text.find_all(attrs={\"style\": \"page-break-after:always\"})\n",
    "            print(all_thematic_breaks[0])\n",
    "        except Exception:\n",
    "            # trying another format\n",
    "            try: \n",
    "                all_thematic_breaks = filing_doc_text.find_all(attrs={\"style\": \"page-break-after: always\"})\n",
    "                print(all_thematic_breaks[0])\n",
    "            except Exception:\n",
    "                # trying another format\n",
    "                try:\n",
    "                    all_thematic_breaks = filing_doc_text.find_all(attrs={\"style\": \"page-break-after: always;\"})\n",
    "                    print(all_thematic_breaks[0])\n",
    "                except Exception:\n",
    "                    # trying another format\n",
    "                    try:\n",
    "                        all_thematic_breaks = filing_doc_text.find_all(\"a\", text = re.compile('Table of Contents'))\n",
    "                        print(all_thematic_breaks[0])\n",
    "                    except Exception:\n",
    "                        print(\"weird page break format, check HTML\")\n",
    "                        break\n",
    "    \n",
    "    # convert all the breaks into strings\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document for being split\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "    if len(all_thematic_breaks) > 0:\n",
    "        \n",
    "        # creates our pattern\n",
    "        regex_delimited_pattern = \"|\".join(map(re.escape, all_thematic_breaks))\n",
    "        \n",
    "        # split the document along the thematic breaks\n",
    "        split_filing_string = re.split(regex_delimited_pattern, filing_doc_string)\n",
    "        \n",
    "        # store the document in the dictionary\n",
    "        master_document_dict[document_id][\"pages_code\"] = split_filing_string\n",
    "        \n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "        master_document_dict[document_id][\"pages_code\"] = [filing_doc_string]\n",
    "        \n",
    "# store the documents in the master_filing_dictionary\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = master_document_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number][\"filing_documents\"]\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # grab all the pages for each document\n",
    "    document_pages = filing_documents[document_id][\"pages_code\"]\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(document_pages)\n",
    "    \n",
    "    # initialize some dictionaries\n",
    "    repaired_pages = {}\n",
    "    normalized_text = {}\n",
    "    \n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser to repair it\n",
    "        page_soup = BeautifulSoup(page, \"html5\")\n",
    "        \n",
    "        # grab the text from each page\n",
    "        page_text = page_soup.html.body.get_text(\" \", strip = True)\n",
    "        \n",
    "        # normalize the text\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize(\"NFKD\", page_text))\n",
    "        \n",
    "        # additional cleaning\n",
    "        page_text_norm = page_text_norm.replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
    "        \n",
    "        # define our page number\n",
    "        page_number = index+1\n",
    "        \n",
    "        # add normalized text to the dictionary\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        \n",
    "        # add the repaired html code to the dictionary\n",
    "        repaired_pages[page_number] = page_soup\n",
    "    \n",
    "    # add the normalized text dictionary to the master filing dictionary\n",
    "    filing_documents[document_id][\"page_normalized_text\"] = normalized_text\n",
    "    \n",
    "    # add the repaired pages to the master filing dictionary\n",
    "    filing_documents[document_id][\"pages_code\"] = repaired_pages\n",
    "    \n",
    "    # add the page numbers we generate\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    filing_documents[document_id][\"page_numbers_generated\"] = gen_page_numbers\n",
    "\n",
    "master_filings_dict[accession_number][\"filing_documents\"] = filing_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FORWARD-LOOKING STATEMENTS References in this Annual Report on Form 10-K to “we,” “us,” “our,” “Ulta Beauty,” the “Company” and similar references mean Ulta Beauty, Inc. and its consolidated subsidiaries, unless otherwise expressly stated or the context otherwise requires. This Annual Report on Form 10-K contains forward-looking statements within the meaning of Section 21E of the Securities Exchange Act of 1934, as amended, and the safe harbor provisions of the Private Securities Litigation Reform Act of 1995, which reflect our current views with respect to, among other things, future events and financial performance. You can identify these forward-looking statements by the use of forward-looking words such as “outlook,” “believes,” “expects,” “plans,” “estimates,” “targets,” “strategies” or other comparable words. Any forward-looking statements contained in this Form 10-K are based upon our historical performance and on current plans, estimates, and expectations. The inclusion of this forward-looking information should not be regarded as a representation by us or any other person that the future plans, estimates, targets, strategies, or expectations contemplated by us will be achieved. Such forward-looking statements are subject to various risks and uncertainties, which include, without limitation: ● The uncertain negative impacts the coronavirus (COVID-19) will have on our business, financial condition, profitability, cash flows and supply chain, as well as consumer spending; ● epidemics, pandemics like COVID-19 or natural disasters that could negatively impact sales; ● changes in the overall level of consumer spending and volatility in the economy; ● our ability to sustain our growth plans and successfully implement our long-range strategic and financial plan; ● our ability to gauge beauty trends and react to changing consumer preferences in a timely manner; ● the possibility that we may be unable to compete effectively in our highly competitive markets; ● our ability to execute our Efficiencies for Growth cost optimization program; ● the possibility that cybersecurity breaches and other disruptions could compromise our information or result in the unauthorized disclosure of confidential information; ● the possibility of material disruptions to our information systems; ● the possibility that the capacity of our distribution and order fulfillment infrastructure and the performance of our newly opened and to be opened distribution centers may not be adequate to support our recent growth and expected future growth plans; ● changes in the wholesale cost of our products; ● the possibility that new store openings and existing locations may be impacted by developer or co-tenant issues; ● our ability to attract and retain key executive personnel; ● our ability to successfully execute our common stock repurchase program or implement future common stock repurchase programs; and ● other risk factors detailed in our public filings with the Securities and Exchange Commission (the SEC), including risk factors contained in Item 1A, “Risk Factors” of this Annual Report on Form 10-K for the year ended February 1, 2020, as such may be amended or supplemented in our subsequently filed Quarterly Reports on Form 10-Q. Except to the extent required by the federal securities laws, we undertake no obligation to publicly update or revise any forward-looking statements, whether as a result of new information, future events or otherwise. 1'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_filings_dict[accession_number][\"filing_documents\"][\"10-K\"][\"page_normalized_text\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/1403568/000155837020003272/0001558370-20-003272.txt'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythondata] *",
   "language": "python",
   "name": "conda-env-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
